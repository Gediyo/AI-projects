{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ7J9RkwfYlx"
      },
      "source": [
        "@author Gediyon M. Girma\n",
        "\n",
        "TD(n) prediction: State value function prediction for a given policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXGe2XLbegbj",
        "outputId": "56154aa6-e681-45ec-a836-9fef2475cb95"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JUQ3Fun1o2uQ"
      },
      "outputs": [],
      "source": [
        "bins = (30, 30)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  low = env.observation_space.low\n",
        "  high = env.observation_space.high\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9F4dDalw65n",
        "outputId": "abc16aaf-8d17-4da3-99bc-72ffbb7f6c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:05:17\n",
            "Episode:  20000  time:  00:10:10\n",
            "Episode:  30000  time:  00:14:56\n",
            "Episode:  40000  time:  00:19:53\n",
            "Episode:  50000  time:  00:24:45\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# TD(n) prediction algorithm\n",
        "\n",
        "alpha = 0.1 # step size for incremental averaging\n",
        "gamma = 1 #discount factor\n",
        "n = 2 # number of steps\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]), np.arange(bins[1]))\n",
        "\n",
        "policy = {}\n",
        "V = {} # state value function\n",
        "\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  V[state] = 0 # initialize the state-value function\n",
        "\n",
        "  policy[state] = np.zeros((env.action_space.n)) # initialize the a random policy\n",
        "  policy[state][np.random.randint(0, env.action_space.n)] = 1 \n",
        "\n",
        "\n",
        "\n",
        "episodes = 5e4\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "  episode_tracker = []\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  episode_tracker.append([0,state])\n",
        "  T = float('inf') # terminal state\n",
        "\n",
        "  t = 0\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if t<T:\n",
        "      action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # select an action\n",
        "\n",
        "      episode_tracker[-1].append(action)\n",
        "\n",
        "      obs, reward, done, info = env.step(action) # taking the action\n",
        "      next_state = discretize_observation(obs) # next state\n",
        "\n",
        "      episode_tracker.append([reward, next_state])\n",
        "\n",
        "      if done:\n",
        "        T = t+1\n",
        "\n",
        "    tau = t - n + 1 # the time step whose state estimate is being updated (going n steps back)\n",
        "\n",
        "    if tau >= 0:\n",
        "      \n",
        "      # compute the return\n",
        "      G = sum([(gamma**(i-tau-1))*episode_tracker[i][0] for i in range(tau+1, min(tau+n,T)+1)]) \n",
        "\n",
        "      if tau + n < T:\n",
        "        G += (gamma**n)*V[episode_tracker[tau+n][1]] # compute the discounted return\n",
        "\n",
        "      # update the action-value function\n",
        "      V[episode_tracker[tau][1]] += alpha * (G - V[episode_tracker[tau][1]]) \n",
        "      \n",
        "    if tau == T-1:  # when the update time reaches terminal state, break\n",
        "      break\n",
        "\n",
        "    state = next_state  # update the state\n",
        "\n",
        "    t += 1 # updating the time- step\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
