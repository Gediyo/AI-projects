{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEcYEwvIhd7R"
      },
      "source": [
        "@author: Gediyon M. Girma\n",
        "\n",
        "Cart-Pole control using TD(0) methods:\n",
        "\n",
        "a. on-policy SARSA\n",
        "\n",
        "b. off-policy Q learning\n",
        "\n",
        "c. off-policy Expected SARSA with an epsilon-greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rs4CACig5PC",
        "outputId": "075376d2-3f49-414b-88d7-8ae749c5160f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHjq429UivOs",
        "outputId": "f560d5ee-a26a-48ba-e64d-fbfe0dfe14d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# discretization of the observation space\n",
        "\n",
        "bins = (20, 20, 20, 20)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "  low = [-4.8, -5, -0.418, -5]\n",
        "  high = [4.8, 5, 0.418, 5]\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkjGAXbIjiD6",
        "outputId": "244f9e6b-55b1-4b1f-f19f-908b3ae693b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:02:02\n",
            "Episode:  20000  time:  00:05:46\n",
            "Episode:  30000  time:  00:10:43\n",
            "Episode:  40000  time:  00:17:08\n",
            "Episode:  50000  time:  00:23:53\n",
            "Episode:  60000  time:  00:30:54\n",
            "Episode:  70000  time:  00:37:55\n",
            "Episode:  80000  time:  00:45:05\n",
            "Episode:  90000  time:  00:52:18\n",
            "Episode:  100000  time:  00:59:31\n",
            "done!\n",
            "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 185, 193, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 183, 186, 200, 200, 200, 200, 200, 176, 200, 200, 200, 200, 200, 200, 200, 183, 200, 200, 174, 200, 200, 200, 193, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 192, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 159, 200, 180, 200, 200, 183, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 187, 200, 200]\n",
            "Average steps to solution:  197.74\n"
          ]
        }
      ],
      "source": [
        "# the on-policy SARSA initialization\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "performance = {}\n",
        "\n",
        "#########  initialization ###########\n",
        "\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]),\n",
        "                          np.arange(bins[1]),\n",
        "                          np.arange(bins[2]),\n",
        "                          np.arange(bins[3]))\n",
        "\n",
        "q = {}\n",
        "policy = {}\n",
        "\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros((env.action_space.n)) # initialize the action-value function\n",
        "\n",
        "  # initializing the epsilon-greedy policy arbitrarly\n",
        "  policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "  policy[state][np.random.randint(env.action_space.n)] += 1-epsilon\n",
        "\n",
        "########## iteration  #############\n",
        "\n",
        "episodes = 1e5\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "  action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # selecting an action\n",
        "  # generating an episodes using a an epsilon greedy policy\n",
        "  for t in range(1,5000):\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = discretize_observation(obs) # next state\n",
        "    next_action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # next action\n",
        "    q[state][action] += alpha * (reward + gamma * q[next_state][next_action] - q[state][action]) # update the action-value function\n",
        "\n",
        "    # update the policy based on the action-value function\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "    policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # resetting the prob. for all action\n",
        "    policy[state][best_action] += 1 - epsilon  # adding 1-epsilon to the best action's probability\n",
        "\n",
        "    state = next_state\n",
        "    action = next_action\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")\n",
        "\n",
        "########## test ##############\n",
        "\n",
        "steps = []\n",
        "\n",
        "for j in range(100):\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs)\n",
        "  for i in range(1,50000):\n",
        "    action = np.argmax(policy[state])\n",
        "    # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs)\n",
        "\n",
        "    if done:\n",
        "      #print (\"done\")\n",
        "      steps.append(i)\n",
        "      break\n",
        "print(steps)\n",
        "avg_step = np.mean(steps)\n",
        "performance[episode] = avg_step\n",
        "print(\"Average steps til termination: \",avg_step)\n",
        "\n",
        "\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMIEDACDHCMw"
      },
      "source": [
        "Off policy Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am5CIHBELXhl",
        "outputId": "c102045b-1105-4d19-b2a4-66d6212d7fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:06:30\n",
            "Episode:  20000  time:  00:14:19\n",
            "Episode:  30000  time:  00:22:14\n",
            "Episode:  40000  time:  00:30:20\n",
            "Episode:  50000  time:  00:38:34\n",
            "Episode:  60000  time:  00:46:51\n",
            "Episode:  70000  time:  00:54:59\n",
            "Episode:  80000  time:  01:03:11\n",
            "Episode:  90000  time:  01:11:24\n",
            "Episode:  100000  time:  01:19:38\n",
            "done!\n",
            "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
            "Average steps to solution:  200.0\n"
          ]
        }
      ],
      "source": [
        "########  initialization  #########\n",
        "\n",
        "\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]),\n",
        "                           np.arange(bins[1]),\n",
        "                           np.arange(bins[2]),\n",
        "                           np.arange(bins[3]))\n",
        "\n",
        "q = {}\n",
        "b = {}  # behavioural policy\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros((env.action_space.n)) # initialize the action-value function\n",
        "\n",
        "  # initializing the epsilon-greedy policy with respect to the action-value function\n",
        "  b[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "  b[state][np.random.randint(env.action_space.n)] += 1-epsilon\n",
        "\n",
        "######## iteration ############\n",
        "\n",
        "episodes = 1e5\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  for t in range(1,1000):\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = b[state]) # select an action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = discretize_observation(obs) # next state\n",
        "\n",
        "    # update the action-value function\n",
        "    q[state][action] += alpha * (reward + gamma * np.max(q[next_state]) - q[state][action]) # update the action-value function\n",
        "\n",
        "    # update the policy based on the action-value function\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "    b[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # resetting the policy for the given state\n",
        "    b[state][best_action] += 1 - epsilon # setting the probability of the best action to 1\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    # check if the episode is done\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")\n",
        "\n",
        "##########   test  ###########\n",
        "\n",
        "steps_to_solution = []\n",
        "\n",
        "for j in range(100):\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs)\n",
        "  for i in range(1,50000):\n",
        "    action = np.argmax(q[state])\n",
        "    # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs)\n",
        "\n",
        "    if done:\n",
        "      #print (\"done\")\n",
        "      steps_to_solution.append(i)\n",
        "      break\n",
        "print(steps_to_solution)\n",
        "avg_step = np.mean(steps_to_solution)\n",
        "print(\"Average steps till termination: \",avg_step)\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RUKPezlHJh3"
      },
      "source": [
        "Off-policy Expected SARSA with an epsilon-greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0lyW8k8HItZ",
        "outputId": "3f5f95ac-e682-4990-f899-4ac1dcd41de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:00:58\n",
            "Episode:  20000  time:  00:01:55\n",
            "Episode:  30000  time:  00:02:52\n",
            "Episode:  40000  time:  00:03:51\n",
            "Episode:  50000  time:  00:04:49\n",
            "Episode:  60000  time:  00:05:46\n",
            "Episode:  70000  time:  00:06:44\n",
            "Episode:  80000  time:  00:07:42\n",
            "Episode:  90000  time:  00:08:38\n",
            "Episode:  100000  time:  00:09:30\n",
            "done!\n",
            "[200, 112, 160, 200, 128, 155, 200, 200, 200, 172, 157, 200, 200, 200, 151, 185, 190, 200, 200, 200, 156, 174, 200, 182, 200, 200, 164, 118, 200, 197, 200, 200, 200, 200, 159, 103, 161, 200, 200, 148, 139, 191, 200, 200, 200, 122, 176, 160, 160, 181, 200, 195, 200, 116, 156, 200, 147, 180, 200, 135, 200, 200, 200, 130, 200, 200, 137, 200, 200, 184, 200, 200, 200, 160, 155, 171, 153, 123, 200, 194, 95, 128, 175, 200, 200, 200, 164, 195, 126, 200, 127, 159, 159, 133, 200, 127, 200, 181, 122, 126]\n",
            "Average steps to solution:  174.54\n"
          ]
        }
      ],
      "source": [
        "#######  initialization   ########\n",
        "\n",
        "# the off-policy SARSA with epsilon greedy control policy\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]),\n",
        "                           np.arange(bins[1]),\n",
        "                           np.arange(bins[2]),\n",
        "                           np.arange(bins[3])\n",
        "                           )\n",
        "\n",
        "q={}\n",
        "policy = {}\n",
        "b = {}  # behavioural policy\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros((env.action_space.n)) # initialize the action-value function\n",
        "\n",
        "  # initializing the epsilon-greedy policy with respect to the action-value function\n",
        "  policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "  policy[state][np.random.randint(env.action_space.n)] += 1-epsilon\n",
        "\n",
        "  # initializing the behaviour policy with equal probability of selecting each action\n",
        "  b[state] = np.full(((env.action_space.n)),1/env.action_space.n)\n",
        "\n",
        "\n",
        "\n",
        "########  Iteration  ###########\n",
        "\n",
        "episodes = 1e5\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  for t in range(1,1000):\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = b[state]) # select an action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = discretize_observation(obs) # next state\n",
        "\n",
        "    # update the action-value function\n",
        "    q[state][action] += alpha * (reward + sum(gamma * policy[next_state][i] * q[next_state][i]\n",
        "                                 for i in range(env.action_space.n)) - q[state][action])\n",
        "\n",
        "    # update the target policy based on the action-value function\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "    policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # resetting the policy for the given state\n",
        "    policy[state][best_action] += 1 - epsilon # setting the probability of the best action to 1\n",
        "\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    # check if the episode is done\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")\n",
        "\n",
        "\n",
        "##########   test  ###########\n",
        "\n",
        "steps_to_solution = []\n",
        "\n",
        "for j in range(100):\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs)\n",
        "  for i in range(1,50000):\n",
        "    action = np.argmax(q[state])\n",
        "    # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs)\n",
        "\n",
        "    if done:\n",
        "      #print (\"done\")\n",
        "      steps_to_solution.append(i)\n",
        "      break\n",
        "print(steps_to_solution)\n",
        "avg_step = np.mean(steps_to_solution)\n",
        "print(\"Average steps till termination: \",avg_step)\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
