{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaHB9ab09hCd"
      },
      "source": [
        "@author Gediyon M. Girma\n",
        "\n",
        "TD(n) or n-step SARSA controllers of TD(2), TD(3), and TD(4) and compare thier performances for MountainCar environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_4MJWR_9Vvw",
        "outputId": "2ddb01d4-e149-4383-8490-60325288533a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbWqqB7MO7Ya"
      },
      "outputs": [],
      "source": [
        "bins = (30, 30)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  low = env.observation_space.low\n",
        "  high = env.observation_space.high\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP8W2gVaQVtz"
      },
      "outputs": [],
      "source": [
        "# TD(n) n-step SARSA on-policy using epsilon-greedy policy\n",
        "\n",
        "alpha = 0.1 #step size for incremental averaging\n",
        "gamma = 1 #discount factor\n",
        "n = 2 # number of steps\n",
        "epsilon = 0.1\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]), np.arange(bins[1]))\n",
        "\n",
        "policy = {}\n",
        "q = {}\n",
        "\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros(env.action_space.n) # initialize the action-value function\n",
        "  policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # initialize the policy\n",
        "  policy[state][np.random.randint(3)] += 1 - epsilon # initialize a random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mNIO-_ZQqgb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def TD_n(n):\n",
        "  print(\"\\n\")\n",
        "  print(\"TD(\",n,\") n-SARSA on-policy training for MountainCar-v0\")\n",
        "  episodes = 5e4\n",
        "  start_timer = time.time()\n",
        "  episode = 1\n",
        "\n",
        "  while episode < episodes:\n",
        "    episode_tracker = []\n",
        "\n",
        "    # reset the environment\n",
        "    obs = env.reset()\n",
        "    state = discretize_observation(obs) # discretize the observation\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # select an action\n",
        "\n",
        "    episode_tracker.append([0,state,action])\n",
        "    T = float('inf') # terminal state\n",
        "\n",
        "    t = 0 #initialize the time-step counter t\n",
        "\n",
        "    while True:\n",
        "      if t<T:\n",
        "\n",
        "        obs, reward, done, info = env.step(action) # taking the action\n",
        "        next_state = discretize_observation(obs) # next state\n",
        "\n",
        "        episode_tracker.append([reward, next_state])\n",
        "\n",
        "        if done:\n",
        "          T = t+1\n",
        "        else:\n",
        "          action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # select an action\n",
        "          episode_tracker[-1].append(action)\n",
        "\n",
        "      tau = t - n + 1 # the time step whose state estimate is being updated (can be thought of us going n steps back)\n",
        "\n",
        "      if tau >= 0:\n",
        "\n",
        "        G = sum([(gamma**(i-tau-1))*episode_tracker[i][0] for i in range(tau + 1, min(tau + n, T))]) # compute the return\n",
        "\n",
        "\n",
        "        if tau + n < T:\n",
        "          G += (gamma**n)*q[episode_tracker[tau + n][1]][episode_tracker[tau + n][2]] # compute the discounted return\n",
        "\n",
        "        q[episode_tracker[tau][1]][episode_tracker[tau][2]] += alpha * (G - q[episode_tracker[tau][1]][episode_tracker[tau][2]]) # update the action-value function\n",
        "\n",
        "        policy[episode_tracker[tau][1]] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # update the policy\n",
        "        best_action = np.argmax(q[episode_tracker[tau][1]])\n",
        "        policy[episode_tracker[tau][1]][best_action] += 1-epsilon\n",
        "\n",
        "      if tau == T-1:  # when the update time reaches terminal state, break\n",
        "        break\n",
        "\n",
        "      state = next_state  # update the state\n",
        "\n",
        "      t += 1 # updating the time- step\n",
        "\n",
        "    episode += 1\n",
        "    if episode % 10000 == 0:\n",
        "      end_timer = time.time()\n",
        "      timer = end_timer - start_timer\n",
        "      elapsed_time_struct = time.gmtime(timer)\n",
        "      formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "      print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "    if episode == episodes:\n",
        "      print(\"done!\")\n",
        "\n",
        "\n",
        "  #test\n",
        "  steps_to_solution = []\n",
        "\n",
        "  for j in range(100):\n",
        "    obs = env.reset()\n",
        "    state = discretize_observation(obs)\n",
        "    for i in range(1,50000):\n",
        "      action = np.argmax(policy[state])\n",
        "      # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      state = discretize_observation(obs)\n",
        "\n",
        "      if done:\n",
        "        #print (\"done\")\n",
        "        steps_to_solution.append(i)\n",
        "        break\n",
        "  print(steps_to_solution)\n",
        "  avg_step = np.mean(steps_to_solution)\n",
        "  print(\"Average steps to solution per 100 episodes: \",avg_step)\n",
        "\n",
        "  # ipythondisplay.clear_output(wait=True)\n",
        "  env.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tSvdFsIR9It",
        "outputId": "be4df679-fa92-4a75-b7c4-4ff584e7d496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TD( 2 ) n-SARSA on-policy training for MountainCar-v0\n",
            "Episode:  10000  time:  00:06:08\n",
            "Episode:  20000  time:  00:11:44\n",
            "Episode:  30000  time:  00:17:12\n",
            "Episode:  40000  time:  00:22:38\n",
            "Episode:  50000  time:  00:28:12\n",
            "done!\n",
            "[148, 149, 156, 153, 119, 150, 148, 155, 150, 152, 153, 128, 150, 128, 151, 150, 151, 148, 147, 193, 128, 149, 200, 148, 149, 200, 150, 150, 193, 149, 152, 119, 150, 149, 128, 172, 151, 128, 150, 152, 200, 152, 152, 153, 150, 123, 153, 152, 146, 123, 149, 150, 118, 123, 151, 149, 124, 147, 151, 117, 150, 151, 123, 155, 148, 152, 200, 117, 125, 150, 117, 150, 150, 148, 200, 117, 127, 117, 125, 149, 154, 152, 152, 129, 150, 150, 200, 149, 153, 117, 146, 151, 147, 150, 152, 124, 119, 153, 149, 151]\n",
            "Average steps to solution per 100 episodes:  147.48\n",
            "\n",
            "\n",
            "TD( 3 ) n-SARSA on-policy training for MountainCar-v0\n",
            "Episode:  10000  time:  00:05:31\n",
            "Episode:  20000  time:  00:10:56\n",
            "Episode:  30000  time:  00:16:31\n",
            "Episode:  40000  time:  00:21:57\n",
            "Episode:  50000  time:  00:27:22\n",
            "done!\n",
            "[157, 114, 114, 156, 148, 114, 117, 114, 116, 116, 148, 149, 147, 158, 115, 143, 150, 149, 152, 115, 113, 148, 117, 163, 115, 113, 142, 115, 117, 115, 153, 115, 146, 157, 148, 115, 155, 115, 115, 144, 114, 113, 154, 153, 115, 140, 116, 146, 146, 117, 114, 116, 114, 157, 147, 115, 115, 116, 159, 156, 116, 146, 113, 115, 142, 116, 116, 164, 114, 116, 116, 114, 113, 154, 114, 145, 116, 148, 115, 114, 113, 115, 114, 147, 115, 148, 113, 146, 155, 116, 114, 114, 115, 115, 116, 147, 116, 115, 163, 157]\n",
            "Average steps to solution per 100 episodes:  129.97\n",
            "\n",
            "\n",
            "TD( 4 ) n-SARSA on-policy training for MountainCar-v0\n",
            "Episode:  10000  time:  00:05:37\n",
            "Episode:  20000  time:  00:10:47\n",
            "Episode:  30000  time:  00:15:55\n",
            "Episode:  40000  time:  00:21:00\n",
            "Episode:  50000  time:  00:26:11\n",
            "done!\n",
            "[164, 188, 165, 178, 165, 164, 192, 167, 191, 185, 191, 190, 193, 166, 174, 188, 165, 192, 190, 162, 197, 187, 165, 166, 170, 190, 164, 165, 168, 110, 166, 168, 165, 194, 173, 168, 167, 170, 167, 166, 191, 192, 186, 163, 175, 165, 195, 168, 165, 166, 186, 168, 190, 171, 165, 188, 166, 114, 167, 194, 189, 165, 188, 165, 187, 196, 200, 185, 193, 175, 164, 170, 188, 110, 169, 166, 166, 196, 165, 114, 165, 110, 168, 187, 166, 165, 165, 189, 193, 168, 165, 110, 169, 190, 167, 188, 170, 191, 191, 167]\n",
            "Average steps to solution per 100 episodes:  172.66\n"
          ]
        }
      ],
      "source": [
        "for n in [2,3,4]:\n",
        "  TD_n(n)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
