{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90oc3wHXCAj"
      },
      "source": [
        "Assignment 3 - Exercise 2\n",
        "\n",
        "TD(n) n-step off-policy tree backup control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ukyPU_95W2_Q"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gU5H51VdXI7K"
      },
      "outputs": [],
      "source": [
        "bins = (30, 30)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  low = env.observation_space.low\n",
        "  high = env.observation_space.high\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp71nGwAX-fo",
        "outputId": "e7c512ec-2f21-490b-af58-d4626155ba3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:07:26\n",
            "Episode:  20000  time:  00:14:50\n",
            "Episode:  30000  time:  00:22:15\n",
            "Episode:  40000  time:  00:29:40\n",
            "Episode:  50000  time:  00:37:06\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# TD(n) n-step tree backup control (off-policy)\n",
        "\n",
        "alpha = 0.1 #step size for incremental averaging\n",
        "gamma = 1 #discount factor\n",
        "n = 2 # number of steps\n",
        "epsilon = 0.1\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]), np.arange(bins[1]))\n",
        "\n",
        "policy = {}\n",
        "q = {}\n",
        "b = {}\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros(env.action_space.n) # initialize the action-value function\n",
        "  policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # initialize the policy\n",
        "  policy[state][np.random.randint(0,env.action_space.n)] += 1-epsilon # initialize a random policy\n",
        "\n",
        "  b[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # initialize the policy\n",
        "  b[state][np.random.randint(0,env.action_space.n)] += 1-epsilon # initialize a random policy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "episodes = 5e4\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "\n",
        "while episode < episodes:\n",
        "  episode_tracker = []\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  action = np.random.choice(np.arange(env.action_space.n), p = b[state])\n",
        "  # action = env.action_space.sample()\n",
        "\n",
        "  episode_tracker.append([0,state,action])\n",
        "  T = float('inf') # terminal state\n",
        "\n",
        "  t = 0 #initialize the time-step counter t\n",
        "\n",
        "  while True:\n",
        "\n",
        "    if t<T:\n",
        "\n",
        "      obs, reward, done, info = env.step(action) # taking the action\n",
        "      next_state = discretize_observation(obs) # next state\n",
        "\n",
        "      episode_tracker.append([reward, next_state]) # append the reward and next state to the episode tracker\n",
        "\n",
        "      if done:\n",
        "        T = t+1\n",
        "      else:\n",
        "        action = np.random.choice(np.arange(env.action_space.n), p = b[state])\n",
        "        # action = env.action_space.sample()\n",
        "        episode_tracker[-1].append(action)  # append the action to the episode tracker\n",
        "\n",
        "    # the time step whose state estimate is being updated\n",
        "    # (can be thought of us going n steps back from current state that is at t+1)\n",
        "    tau = t - n + 1\n",
        "\n",
        "    if tau >= 0:\n",
        "\n",
        "      if t+1 >= T:\n",
        "        G = episode_tracker[T][0] # terminal state reward\n",
        "\n",
        "      else:\n",
        "\n",
        "        G =  episode_tracker[t+1][0] + gamma * sum(policy[episode_tracker[t+1][1]][i] *\n",
        "                                                   q[episode_tracker[t+1][1]][i] \n",
        "                                                   for i in range(env.action_space.n))\n",
        "\n",
        "      for k in reversed(range(tau+1, min(t,T-1)+1)):\n",
        "        r_k = episode_tracker[k][0]\n",
        "        s_k = episode_tracker[k][1]\n",
        "        a_k = episode_tracker[k][2]\n",
        "\n",
        "        G = r_k + (gamma * sum(policy[s_k][i] * q[s_k][i] for i in range(env.action_space.n) \n",
        "                               if i != a_k)) + (gamma * policy[s_k][a_k] * G)\n",
        "\n",
        "        # updating the state-action value function\n",
        "\n",
        "      s = episode_tracker[tau][1]\n",
        "      a = episode_tracker[tau][2]\n",
        "\n",
        "      q[s][a] += alpha * (G - q[s][a])\n",
        "\n",
        "\n",
        "      # updating the target policy\n",
        "      policy[s] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "      best_action = np.argmax(q[s])\n",
        "      policy[s][best_action] += 1-epsilon\n",
        "\n",
        "\n",
        "      # updating the backup policy\n",
        "      b[s] = np.full(((env.action_space.n)),(5*epsilon)/env.action_space.n)\n",
        "      best_action = np.argmax(q[s])\n",
        "      b[s][best_action] += 1- (5*epsilon)\n",
        "\n",
        "    if tau == T-1:  # when the update time reaches terminal state, break\n",
        "      break\n",
        "\n",
        "    state = next_state  # update the state\n",
        "\n",
        "    t += 1 # updating the time- step\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrsCdt_VQFOS",
        "outputId": "01701716-de06-4062-f829-9f2ebab025a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[125, 162, 166, 166, 128, 164, 129, 128, 163, 127, 166, 125, 162, 163, 160, 128, 161, 164, 161, 164, 162, 163, 131, 126, 129, 164, 161, 165, 161, 129, 163, 123, 163, 161, 165, 127, 162, 186, 162, 128, 128, 160, 161, 127, 127, 186, 166, 129, 172, 132, 163, 162, 119, 128, 129, 129, 128, 129, 127, 178, 174, 130, 160, 160, 127, 162, 129, 160, 126, 132, 126, 128, 129, 128, 128, 164, 186, 129, 163, 160, 164, 126, 163, 128, 130, 126, 129, 127, 129, 162, 129, 186, 164, 130, 124, 165, 160, 128, 128, 129]\n",
            "Average steps to solution per 100 episodes:  146.51\n"
          ]
        }
      ],
      "source": [
        "  #test\n",
        "  steps_to_solution = []\n",
        "\n",
        "  for j in range(100):\n",
        "    obs = env.reset()\n",
        "    state = discretize_observation(obs)\n",
        "    for i in range(1,50000):\n",
        "      action = np.argmax(policy[state])\n",
        "      # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      state = discretize_observation(obs)\n",
        "\n",
        "      if done:\n",
        "        #print (\"done\")\n",
        "        steps_to_solution.append(i)\n",
        "        break\n",
        "  print(steps_to_solution)\n",
        "  avg_step = np.mean(steps_to_solution)\n",
        "  print(\"Average steps to solution per 100 episodes: \",avg_step)\n",
        "\n",
        "  # ipythondisplay.clear_output(wait=True)\n",
        "  env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
