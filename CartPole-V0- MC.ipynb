{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8EXb_zQHHPC"
      },
      "source": [
        "@ author: Gediyon M. Girma\n",
        "CartPole-V0 control problem solved with MonteCarlo method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aZEWzDnmXnBa"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "djojM8nMYTrY"
      },
      "outputs": [],
      "source": [
        "%pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gvZnmgMWYV45"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ftWl23gCYtya",
        "outputId": "84c8e58f-cad4-46b7-e28d-59b06267070c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuH6DskPYYCP",
        "outputId": "63cfa4bb-46e1-4665-dd00-bd17827c553a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7dfe7cd8b0d0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "XOW8HPVXYZxU",
        "outputId": "25aad458-2cff-45b3-e548-40d309c0328d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iterations that were run: 32\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "\n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "WKsP0moS9VZu"
      },
      "outputs": [],
      "source": [
        "# part 1: initialization\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# discritizing function for the observation space\n",
        "\n",
        "def discretize(value,min_value,max_value,num_bins):\n",
        "  \"\"\"\n",
        "  Discretizes a value using a uniform grid, clamping values outside of the grid range.\n",
        "\n",
        "  Args:\n",
        "    value: The value to discretize.\n",
        "    min_value: The minimum value of the grid.\n",
        "    max_value: The maximum value of the grid.\n",
        "    num_bins: The number of bins to use.\n",
        "\n",
        "  Returns:\n",
        "    The discretized value.\n",
        "  \"\"\"\n",
        "  value = min(max(value, min_value), max_value)\n",
        "\n",
        "\n",
        "  bin_size = (max_value - min_value) / num_bins\n",
        "  bin_index = int((value - min_value) / bin_size)\n",
        "\n",
        "  return bin_index\n",
        "\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  cart_position, cart_velocity, pole_angle, pole_velocity = observation\n",
        "\n",
        "  state = [discretize(cart_position, -4.8, 4.8, 9),\n",
        "           discretize(cart_velocity, -5, 5, 9),\n",
        "           discretize(pole_angle, -0.418, 0.418, 9),\n",
        "           discretize(pole_velocity, -5, 5, 9)\n",
        "           ]\n",
        "  return tuple(state)\n",
        "\n",
        "\n",
        "# initializing the action-value function\n",
        "\n",
        "\n",
        "# defining the number of bins and therefore the vlaues of the the observation space\n",
        "cart_position_bins = 10\n",
        "cart_velocity_bins = 10\n",
        "pole_angle_bins = 10\n",
        "pole_velocity_bins = 10\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product([x for x in range(cart_position_bins)],\n",
        "                                [x for x in range(cart_velocity_bins)],\n",
        "                                [x for x in range(pole_angle_bins)],\n",
        "                                [x for x in range(pole_velocity_bins)])\n",
        "\n",
        "# initialize the action-value function\n",
        "q = {}\n",
        "c = {}\n",
        "policy = {}\n",
        "b = {} # behaviour policy\n",
        "\n",
        "for state in states:\n",
        "  c[state] = np.zeros((env.action_space.n)) # initialize the sum for the s-a pair\n",
        "  b[state] = np.full(((env.action_space.n)),1/env.action_space.n) # initialize the behaviour policy\n",
        "  q[state] = np.random.random((env.action_space.n)) # initialize the action-value function\n",
        "  policy[state] = np.zeros((env.action_space.n)) # setting zero probabilities for all actions\n",
        "\n",
        "  best_action = np.argmax(q[state])  # selecting the best action based on the action-value function\n",
        "  policy[state][best_action] = 1  # setting 1 to the probability of the best action\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XGp4dFC39Z",
        "outputId": "9cf86159-4e93-4aad-c84e-179e5dc4c937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:02:31\n",
            "Episode:  20000  time:  00:05:03\n",
            "Episode:  30000  time:  00:07:34\n",
            "Episode:  40000  time:  00:10:05\n",
            "Episode:  50000  time:  00:12:36\n",
            "Episode:  60000  time:  00:15:08\n",
            "Episode:  70000  time:  00:17:38\n",
            "Episode:  80000  time:  00:20:10\n",
            "Episode:  90000  time:  00:22:41\n",
            "Episode:  100000  time:  00:25:12\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# part 2 - Iteration\n",
        "\n",
        "start_timer = time.time() # starting the timer\n",
        "\n",
        "\n",
        "epsilon = 0.1\n",
        "\n",
        "episodes = 1e5\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  # initializing a dictionary to save the episode's trajectory\n",
        "  episode_trajectory = [[0, state]] # starting reward (0) and state\n",
        "\n",
        "  # generating an episodes using a an epsilon greedy policy\n",
        "  for t in range(1,50000):\n",
        "\n",
        "    # selecting action for the episode based on b (e-soft policy version of our target policy)\n",
        "    rnd_num = np.random.uniform(0,1) # generating a random number\n",
        "    cumlative_action_prob = np.cumsum(b[state])  # cumlative action probabilities\n",
        "    action = np.argmax(cumlative_action_prob > rnd_num) # selecting the action based on the random number\n",
        "\n",
        "    episode_trajectory[-1].append(action) # appending the action to the episode trajectory\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs) # discretize the observation\n",
        "    episode_trajectory.append([reward,state]) # append the reward and the state to the episode trajectory\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  # initializing the return and the weight\n",
        "  G = 0\n",
        "  W = 1\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # updating the action-value function\n",
        "  for t in range(len(episode_trajectory)-2,-1,-1):\n",
        "    G = gamma*G  + episode_trajectory[t+1][0]\n",
        "    state = episode_trajectory[t][1]\n",
        "    action = episode_trajectory[t][2]\n",
        "    c[state][action] += W  # updating the sum for the s-a pair\n",
        "    q[state][action] += (W/c[state][action]) * (G - q[state][action]) # incremental averaging\n",
        "\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "\n",
        "    policy[state] = np.zeros(env.action_space.n) # resetting the policy for the given state\n",
        "    policy[state][best_action] = 1 # setting the probability of the best action to 1\n",
        "\n",
        "\n",
        "    if action != best_action:\n",
        "      break\n",
        "    W = W/b[state][action]\n",
        "\n",
        "\n",
        "  for state in states:\n",
        "    best_action = np.argmax(q[state])\n",
        "    b[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "    b[state][best_action] += (1-epsilon)\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLYhCjxo9S47",
        "outputId": "4143e2fa-999e-463f-81f2-5e735318ab80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 reward:  0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 1 reward:  1.0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 2 reward:  1.0  state:  (4, 4, 4, 3)  action= 0\n",
            "step 3 reward:  1.0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 4 reward:  1.0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 5 reward:  1.0  state:  (4, 5, 4, 3)  action= 0\n",
            "step 6 reward:  1.0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 7 reward:  1.0  state:  (4, 5, 4, 3)  action= 0\n",
            "step 8 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 9 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 10 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 11 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 12 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 13 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 14 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 15 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 16 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 17 reward:  1.0  state:  (4, 4, 2, 4)  action= 0\n",
            "step 18 reward:  1.0  state:  (4, 4, 2, 4)  action= 0\n",
            "step 19 reward:  1.0  state:  (4, 4, 2, 4)  action= 0\n",
            "step 20 reward:  1.0  state:  (4, 4, 2, 4)  action= 0\n",
            "step 21 reward:  1.0  state:  (4, 3, 2, 4)  action= 0\n",
            "step 22 reward:  1.0  state:  (4, 3, 2, 5)  action= 1\n",
            "step 23 reward:  1.0  state:  (4, 4, 2, 4)  action= 0\n",
            "step 24 reward:  1.0  state:  (4, 3, 2, 4)  action= 0\n",
            "step 25 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 26 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 27 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 28 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 29 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 30 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 31 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 32 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 33 reward:  1.0  state:  (4, 3, 4, 5)  action= 1\n",
            "step 34 reward:  1.0  state:  (4, 3, 4, 4)  action= 1\n",
            "step 35 reward:  1.0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 36 reward:  1.0  state:  (4, 4, 4, 4)  action= 1\n",
            "step 37 reward:  1.0  state:  (4, 4, 4, 3)  action= 0\n",
            "step 38 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 39 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 40 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 41 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 42 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 43 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 44 reward:  1.0  state:  (4, 4, 3, 4)  action= 1\n",
            "step 45 reward:  1.0  state:  (4, 4, 3, 3)  action= 0\n",
            "step 46 reward:  1.0  state:  (4, 4, 2, 3)  action= 0\n",
            "step 47 reward:  1.0  state:  (4, 4, 2, 4)  action= 0\n",
            "step 48 reward:  1.0  state:  (4, 3, 2, 4)  action= 0\n",
            "step 49 reward:  1.0  state:  (4, 3, 2, 4)  action= 0\n",
            "step 50 reward:  1.0  state:  (4, 3, 2, 4)  action= 0\n",
            "step 51 reward:  1.0  state:  (4, 3, 2, 5)  action= 1\n",
            "step 52 reward:  1.0  state:  (4, 3, 2, 4)  action= 0\n",
            "step 53 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 54 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 55 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 56 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 57 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 58 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 59 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 60 reward:  1.0  state:  (4, 3, 3, 5)  action= 1\n",
            "step 61 reward:  1.0  state:  (4, 3, 3, 4)  action= 0\n",
            "step 62 reward:  1.0  state:  (4, 3, 4, 4)  action= 1\n",
            "step 63 reward:  1.0  state:  (4, 3, 4, 4)  action= 1\n",
            "step 64 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 65 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 66 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 67 reward:  1.0  state:  (3, 2, 4, 5)  action= 1\n",
            "step 68 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 69 reward:  1.0  state:  (3, 3, 4, 5)  action= 0\n",
            "step 70 reward:  1.0  state:  (3, 2, 4, 5)  action= 1\n",
            "step 71 reward:  1.0  state:  (3, 2, 5, 5)  action= 1\n",
            "step 72 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 73 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 74 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 75 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 76 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 77 reward:  1.0  state:  (3, 4, 5, 3)  action= 1\n",
            "step 78 reward:  1.0  state:  (3, 4, 4, 3)  action= 0\n",
            "step 79 reward:  1.0  state:  (3, 4, 4, 3)  action= 0\n",
            "step 80 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 81 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 82 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 83 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 84 reward:  1.0  state:  (3, 3, 4, 5)  action= 0\n",
            "step 85 reward:  1.0  state:  (3, 2, 4, 5)  action= 1\n",
            "step 86 reward:  1.0  state:  (3, 3, 4, 5)  action= 0\n",
            "step 87 reward:  1.0  state:  (3, 2, 5, 5)  action= 1\n",
            "step 88 reward:  1.0  state:  (3, 3, 5, 5)  action= 1\n",
            "step 89 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 90 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 91 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 92 reward:  1.0  state:  (3, 3, 5, 4)  action= 1\n",
            "step 93 reward:  1.0  state:  (3, 4, 5, 3)  action= 1\n",
            "step 94 reward:  1.0  state:  (3, 4, 5, 3)  action= 1\n",
            "step 95 reward:  1.0  state:  (3, 4, 5, 3)  action= 1\n",
            "step 96 reward:  1.0  state:  (3, 4, 4, 3)  action= 0\n",
            "step 97 reward:  1.0  state:  (3, 4, 4, 3)  action= 0\n",
            "step 98 reward:  1.0  state:  (3, 4, 4, 3)  action= 0\n",
            "step 99 reward:  1.0  state:  (3, 4, 4, 4)  action= 1\n",
            "step 100 reward:  1.0  state:  (3, 4, 4, 3)  action= 0\n",
            "step 101 reward:  1.0  state:  (3, 4, 3, 4)  action= 0\n",
            "step 102 reward:  1.0  state:  (3, 3, 3, 4)  action= 0\n",
            "step 103 reward:  1.0  state:  (3, 3, 3, 4)  action= 0\n",
            "step 104 reward:  1.0  state:  (3, 3, 3, 4)  action= 0\n",
            "step 105 reward:  1.0  state:  (3, 3, 3, 5)  action= 1\n",
            "step 106 reward:  1.0  state:  (3, 3, 3, 4)  action= 0\n",
            "step 107 reward:  1.0  state:  (3, 3, 4, 4)  action= 0\n",
            "step 108 reward:  1.0  state:  (3, 3, 4, 5)  action= 0\n",
            "step 109 reward:  1.0  state:  (3, 2, 4, 5)  action= 1\n",
            "step 110 reward:  1.0  state:  (3, 3, 4, 5)  action= 0\n",
            "step 111 reward:  1.0  state:  (3, 2, 4, 5)  action= 1\n",
            "step 112 reward:  1.0  state:  (3, 3, 4, 5)  action= 0\n",
            "step 113 reward:  1.0  state:  (3, 2, 5, 5)  action= 1\n",
            "step 114 reward:  1.0  state:  (2, 3, 5, 5)  action= 1\n",
            "step 115 reward:  1.0  state:  (2, 3, 5, 5)  action= 1\n",
            "step 116 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 117 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 118 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 119 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 120 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 121 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 122 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 123 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 124 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 125 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 126 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 127 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 128 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 129 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 130 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 131 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 132 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 133 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 134 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 135 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 136 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 137 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 138 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 139 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 140 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 141 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 142 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 143 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 144 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 145 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 146 reward:  1.0  state:  (2, 3, 5, 4)  action= 1\n",
            "step 147 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 148 reward:  1.0  state:  (2, 3, 5, 5)  action= 1\n",
            "step 149 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 150 reward:  1.0  state:  (2, 3, 6, 5)  action= 1\n",
            "step 151 reward:  1.0  state:  (2, 3, 6, 4)  action= 1\n",
            "step 152 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 153 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 154 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 155 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 156 reward:  1.0  state:  (2, 4, 6, 3)  action= 0\n",
            "step 157 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 158 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 159 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 160 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 161 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 162 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 163 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 164 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 165 reward:  1.0  state:  (2, 5, 5, 3)  action= 0\n",
            "step 166 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 167 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 168 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 169 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 170 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 171 reward:  1.0  state:  (2, 4, 6, 5)  action= 1\n",
            "step 172 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 173 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 174 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 175 reward:  1.0  state:  (2, 5, 6, 4)  action= 1\n",
            "step 176 reward:  1.0  state:  (2, 5, 6, 4)  action= 1\n",
            "step 177 reward:  1.0  state:  (2, 5, 6, 3)  action= 1\n",
            "step 178 reward:  1.0  state:  (2, 5, 6, 3)  action= 1\n",
            "step 179 reward:  1.0  state:  (2, 5, 5, 3)  action= 0\n",
            "step 180 reward:  1.0  state:  (2, 5, 5, 3)  action= 0\n",
            "step 181 reward:  1.0  state:  (2, 5, 5, 4)  action= 1\n",
            "step 182 reward:  1.0  state:  (2, 5, 5, 3)  action= 0\n",
            "step 183 reward:  1.0  state:  (2, 5, 5, 4)  action= 1\n",
            "step 184 reward:  1.0  state:  (2, 5, 5, 3)  action= 0\n",
            "step 185 reward:  1.0  state:  (2, 5, 4, 4)  action= 0\n",
            "step 186 reward:  1.0  state:  (2, 5, 4, 4)  action= 0\n",
            "step 187 reward:  1.0  state:  (2, 4, 4, 4)  action= 0\n",
            "step 188 reward:  1.0  state:  (2, 4, 4, 4)  action= 0\n",
            "step 189 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 190 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 191 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 192 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 193 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 194 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 195 reward:  1.0  state:  (2, 4, 5, 4)  action= 0\n",
            "step 196 reward:  1.0  state:  (2, 4, 5, 5)  action= 1\n",
            "step 197 reward:  1.0  state:  (2, 4, 6, 4)  action= 1\n",
            "step 198 reward:  1.0  state:  (2, 5, 6, 4)  action= 1\n",
            "step 199 reward:  1.0  state:  (2, 5, 6, 4)  action= 1\n",
            "done\n",
            "Iterations that were run: 199\n"
          ]
        }
      ],
      "source": [
        "\n",
        "obs = env.reset()\n",
        "state = discretize_observation(obs)\n",
        "# prev_screen = env.render(mode='rgb_array')\n",
        "# plt.imshow(prev_screen)\n",
        "reward = 0\n",
        "for i in range(50000):\n",
        "  action = np.argmax(policy[state])\n",
        "  print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  # print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  state = discretize_observation(obs)\n",
        "  # screen = env.render(mode='rgb_array')\n",
        "\n",
        "  # plt.imshow(screen)\n",
        "  # ipythondisplay.clear_output(wait=True)\n",
        "  # ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    print (\"done\")\n",
        "    break\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
