{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lHXB7XpFOSB"
      },
      "source": [
        "@author: Gediyon M. Girma\n",
        "\n",
        "MC controller with exploratory start for MountainCar-V0 environment.\n",
        "\n",
        "prblem: Mountain Car\n",
        "\n",
        "*   reward: The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalized with a reward of -1 for each timestep.\n",
        "*   observation: ndarray of size (2,) where index 0 is position in range of  [-1.2, 0.6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ct_7H3lkbfmM"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2H_YB5rbVbg",
        "outputId": "0793cea7-95c4-4f70-fb73-62ab2fa12914"
      },
      "outputs": [],
      "source": [
        "# loading the environment\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xoX4cYCCn9rf"
      },
      "outputs": [],
      "source": [
        "# discretization of the observation space\n",
        "\n",
        "bins = (30, 30)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "  low = env.observation_space.low\n",
        "  high = env.observation_space.high\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Z9LW8f7i9TL7"
      },
      "outputs": [],
      "source": [
        "# initializing\n",
        "gamma = 1 # discount factor\n",
        "alpha = 0.1 # learning rate\n",
        "epsilon = 0.1\n",
        "q = {} # action value function\n",
        "policy = {} # target policy\n",
        "counter = {}\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]), np.arange(bins[1]))\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros((env.action_space.n)) # initialize the action-value function as zero\n",
        "\n",
        "  policy[state] = np.zeros((env.action_space.n)) # initialize the policy\n",
        "  policy[state][np.random.randint(3)] += 1\n",
        "\n",
        "  counter[state] = np.zeros((env.action_space.n))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTWpHWjl4Rb6",
        "outputId": "7cf42f14-8dfe-499b-b7aa-d7d8a7bef7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:06:00\n",
            "Episode:  20000  time:  00:11:46\n",
            "Episode:  30000  time:  00:17:31\n",
            "Episode:  40000  time:  00:23:00\n",
            "Episode:  50000  time:  00:28:33\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# on-policy MC with exploring start\n",
        "# Iteration\n",
        "\n",
        "start_timer = time.time() # starting the timer\n",
        "\n",
        "episodes = 5e4\n",
        "episode = 1\n",
        "\n",
        "while episode < episodes:\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  # Exploratory start\n",
        "  obs = [np.random.uniform(env.observation_space.low[i], env.observation_space.high[i]) for i in range(2)]\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "  env.state = obs # set the state of the environment to exploratory start\n",
        "  action = env.action_space.sample() # sample an action from the action space\n",
        "\n",
        "\n",
        "  # initializing a dictionary to save the episode's trajectory\n",
        "  episode_trajectory = [[0, state, action]] # starting reward (0) and state-action pair\n",
        "\n",
        "  # generating an episodes using a an epsilon greedy policy\n",
        "  while True:\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "    if done:\n",
        "      episode_trajectory.append([reward, state, None])\n",
        "      break\n",
        "\n",
        "    # selecting the action based on the target policy\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = policy[state])\n",
        "\n",
        "\n",
        "    # appending the action to the episode trajectory\n",
        "    episode_trajectory.append([reward, state, action])\n",
        "\n",
        "\n",
        "  # initializing the return and the weight\n",
        "  G = 0\n",
        "\n",
        "  # updating the action-value function\n",
        "  for t in reversed(range(len(episode_trajectory) - 1)):\n",
        "\n",
        "    G = gamma*G  + episode_trajectory[t+1][0]\n",
        "\n",
        "    state = episode_trajectory[t][1]\n",
        "    action = episode_trajectory[t][2]\n",
        "\n",
        "    # returns[state][action].append(G)\n",
        "\n",
        "    counter[state][action] += 1\n",
        "\n",
        "    q[state][action] += (G-q[state][action])/(counter[state][action]) # incremental averaging\n",
        "\n",
        "    # updating the policy\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "    policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # resetting the policy for the given state\n",
        "    policy[state][best_action] += 1 - epsilon # setting the probability of the best action to 1\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYI_9Bc-VwGT",
        "outputId": "06f5beb5-b0cb-4d09-e167-5e3de80138e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
            "Average steps to solution:  200.0\n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "steps_to_solution = []\n",
        "\n",
        "for j in range(100):\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs)\n",
        "  for i in range(1,50000):\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = policy[state])\n",
        "    # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs)\n",
        "\n",
        "    if done:\n",
        "      #print (\"done\")\n",
        "      steps_to_solution.append(i)\n",
        "      break\n",
        "print(steps_to_solution)\n",
        "avg_step = np.mean(steps_to_solution)\n",
        "print(\"Average steps to solution: \",avg_step)\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
