{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "@author Gediyon M. Girma\n",
        "\n",
        "TD(n) or n-step SARSA controllers of TD(2), TD(3), and TD(4) and compare thier performances for MountainCar environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNkz8USJv_72",
        "outputId": "8f8d4235-3345-4121-8fc7-e1422dfd1efc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zt2LhKc_51EK"
      },
      "outputs": [],
      "source": [
        "bins = (30, 30)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  low = env.observation_space.low\n",
        "  high = env.observation_space.high\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KYgHfXxf5634"
      },
      "outputs": [],
      "source": [
        "# TD(n) n-step SARSA on-policy using epsilon-greedy policy\n",
        "def TD_n(n):\n",
        "  alpha = 0.1 #step size for incremental averaging\n",
        "  gamma = 1 #discount factor\n",
        "  epsilon = 0.1\n",
        "\n",
        "  # formaulate the state space with every combination of the discritsized elements of the states\n",
        "  states = itertools.product(np.arange(bins[0]), np.arange(bins[1]))\n",
        "\n",
        "  policy = {}\n",
        "  q = {}\n",
        "\n",
        "\n",
        "\n",
        "  for state in states:\n",
        "    q[state] = np.zeros(env.action_space.n) # initialize the action-value function\n",
        "    policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # initialize the policy\n",
        "    policy[state][np.random.randint(3)] += 1 - epsilon # initialize a random policy\n",
        "\n",
        "\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"TD(\",n,\") n-SARSA on-policy training for MountainCar-v0\")\n",
        "  episodes = 5e4\n",
        "  start_timer = time.time()\n",
        "  episode = 1\n",
        "\n",
        "  while episode < episodes:\n",
        "    episode_tracker = []\n",
        "\n",
        "    # reset the environment\n",
        "    obs = env.reset()\n",
        "    state = discretize_observation(obs) # discretize the observation\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # select an action\n",
        "\n",
        "    episode_tracker.append([0,state,action])\n",
        "    T = float('inf') # terminal state\n",
        "\n",
        "    t = 0 #initialize the time-step counter t\n",
        "\n",
        "    while True:\n",
        "      if t<T:\n",
        "\n",
        "        obs, reward, done, info = env.step(action) # taking the action\n",
        "        next_state = discretize_observation(obs) # next state\n",
        "\n",
        "        episode_tracker.append([reward, next_state])\n",
        "\n",
        "        if done:\n",
        "          T = t+1\n",
        "        else:\n",
        "          action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # select an action\n",
        "          episode_tracker[-1].append(action)\n",
        "\n",
        "      tau = t - n + 1 # the time step whose state estimate is being updated (going n steps back)\n",
        "\n",
        "      if tau >= 0:\n",
        "        # compute the return\n",
        "        G = sum([(gamma**(i-tau-1))*episode_tracker[i][0] for i in range(tau + 1, min(tau + n, T))]) \n",
        "\n",
        "\n",
        "        if tau + n < T:\n",
        "          # compute the discounted return\n",
        "          G += (gamma**n)*q[episode_tracker[tau + n][1]][episode_tracker[tau + n][2]] \n",
        "          \n",
        "        s = episode_tracker[tau][1] # current state for update\n",
        "        a = episode_tracker[tau][2] # current action for update\n",
        "\n",
        "        q[s][a] += alpha * (G - q[s][a]) # update the action-value function\n",
        "\n",
        "        policy[s] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # update the policy\n",
        "        best_action = np.argmax(q[s])\n",
        "        policy[s][best_action] += 1-epsilon\n",
        "\n",
        "      if tau == T-1:  # when the update time reaches terminal state, break\n",
        "        break\n",
        "\n",
        "      state = next_state  # update the state\n",
        "\n",
        "      t += 1 # updating the time- step\n",
        "\n",
        "    episode += 1\n",
        "    if episode % 10000 == 0:\n",
        "      end_timer = time.time()\n",
        "      timer = end_timer - start_timer\n",
        "      elapsed_time_struct = time.gmtime(timer)\n",
        "      formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "      print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "    if episode == episodes:\n",
        "      print(\"done!\")\n",
        "\n",
        "\n",
        "  #test\n",
        "  steps_to_solution = []\n",
        "\n",
        "  for j in range(100):\n",
        "    obs = env.reset()\n",
        "    state = discretize_observation(obs)\n",
        "    for i in range(1,50000):\n",
        "      action = np.argmax(policy[state])\n",
        "      # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      state = discretize_observation(obs)\n",
        "\n",
        "      if done:\n",
        "        #print (\"done\")\n",
        "        steps_to_solution.append(i)\n",
        "        break\n",
        "  print(steps_to_solution)\n",
        "  avg_step = np.mean(steps_to_solution)\n",
        "  print(\"Average steps to solution per 100 episodes: \",avg_step)\n",
        "\n",
        "  # ipythondisplay.clear_output(wait=True)\n",
        "  env.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebULWLMV5-9y",
        "outputId": "50c49715-92c1-4c6a-e74e-bcbd6f29aaf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TD( 2 ) n-SARSA on-policy training for MountainCar-v0\n",
            "Episode:  10000  time:  00:06:15\n",
            "Episode:  20000  time:  00:11:59\n",
            "Episode:  30000  time:  00:17:33\n",
            "Episode:  40000  time:  00:23:00\n",
            "Episode:  50000  time:  00:28:20\n",
            "done!\n",
            "[146, 186, 137, 184, 184, 185, 189, 137, 137, 145, 144, 146, 190, 144, 150, 137, 184, 137, 144, 183, 186, 146, 191, 137, 138, 189, 188, 186, 137, 184, 182, 146, 146, 185, 187, 190, 189, 137, 186, 182, 144, 188, 138, 146, 138, 137, 137, 189, 154, 185, 189, 188, 149, 188, 190, 182, 189, 150, 146, 189, 137, 182, 190, 190, 137, 136, 136, 188, 185, 185, 138, 148, 190, 143, 189, 137, 190, 190, 188, 191, 186, 184, 184, 190, 185, 136, 138, 136, 188, 184, 189, 137, 188, 137, 137, 189, 138, 184, 189, 139]\n",
            "Average steps to solution per 100 episodes:  166.25\n"
          ]
        }
      ],
      "source": [
        "TD_n(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR-bBXEaAKbZ",
        "outputId": "7d0846ee-9e77-4ed3-b5c9-0c6aed963277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TD( 3 ) n-SARSA on-policy training for MountainCar-v0\n",
            "Episode:  10000  time:  00:05:57\n",
            "Episode:  20000  time:  00:11:19\n",
            "Episode:  30000  time:  00:16:47\n",
            "Episode:  40000  time:  00:22:12\n",
            "Episode:  50000  time:  00:27:34\n",
            "done!\n",
            "[133, 134, 134, 133, 133, 133, 200, 133, 132, 200, 133, 134, 133, 200, 200, 132, 133, 133, 200, 200, 135, 133, 134, 133, 135, 131, 133, 132, 133, 134, 133, 134, 200, 133, 133, 133, 133, 134, 134, 133, 133, 134, 134, 131, 133, 134, 132, 133, 133, 134, 134, 132, 134, 200, 133, 132, 132, 134, 133, 134, 133, 133, 134, 200, 132, 132, 132, 132, 132, 135, 134, 133, 133, 135, 134, 132, 133, 200, 136, 133, 200, 132, 134, 133, 135, 200, 132, 132, 133, 200, 134, 135, 132, 134, 133, 133, 135, 133, 200, 133]\n",
            "Average steps to solution per 100 episodes:  142.56\n"
          ]
        }
      ],
      "source": [
        "TD_n(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOviUBEDLC7c",
        "outputId": "4867fb27-eb6a-49e1-c46e-0a1bd743bd3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TD( 4 ) n-SARSA on-policy training for MountainCar-v0\n",
            "Episode:  10000  time:  00:05:53\n",
            "Episode:  20000  time:  00:11:29\n",
            "Episode:  30000  time:  00:17:01\n",
            "Episode:  40000  time:  00:22:32\n",
            "Episode:  50000  time:  00:28:06\n",
            "done!\n",
            "[200, 200, 147, 200, 200, 200, 200, 200, 200, 200, 185, 200, 200, 200, 200, 200, 184, 200, 147, 200, 182, 200, 200, 200, 200, 200, 200, 200, 200, 184, 200, 200, 184, 185, 200, 200, 200, 185, 200, 200, 200, 200, 161, 200, 200, 184, 154, 200, 200, 184, 200, 184, 200, 200, 200, 200, 200, 200, 200, 184, 200, 200, 200, 200, 200, 200, 200, 183, 198, 200, 184, 200, 184, 200, 200, 148, 200, 200, 200, 200, 147, 200, 200, 184, 200, 200, 200, 147, 184, 200, 200, 183, 200, 185, 184, 200, 200, 200, 200, 200]\n",
            "Average steps to solution per 100 episodes:  193.45\n"
          ]
        }
      ],
      "source": [
        "TD_n(4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
