{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lHXB7XpFOSB"
      },
      "source": [
        "@author: Gediyon M. Girma\n",
        "\n",
        "off policy MC controller via importance sampling for MountainCar environment\n",
        "\n",
        "prblem: Mountain Car\n",
        "\n",
        "\n",
        "*   reward: The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalized with a reward of -1 for each timestep.\n",
        "*   observation: ndarray of size (2,) where index 0 is position in range of  [-1.2, 0.6]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct_7H3lkbfmM",
        "outputId": "8b840147-c4c0-4f4c-ca89-3333ec7937cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2H_YB5rbVbg",
        "outputId": "beedff36-c087-4d1e-fe7e-8ee29a064812"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# loading the environment\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoX4cYCCn9rf",
        "outputId": "1f76ea7e-a974-4dfa-9583-d5a137adf9aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# discretization of the observation space\n",
        "\n",
        "position_bins = 10\n",
        "velocity_bins = 10\n",
        "\n",
        "def discretize(value,min_value,max_value,num_bins):\n",
        "  \"\"\"\n",
        "  Discretizes a value using a uniform grid, clamping values outside of the grid range.\n",
        "\n",
        "  Args:\n",
        "    value: The value to discretize.\n",
        "    min_value: The minimum value of the grid.\n",
        "    max_value: The maximum value of the grid.\n",
        "    num_bins: The number of bins to use.\n",
        "\n",
        "  Returns:\n",
        "    The discretized value.\n",
        "  \"\"\"\n",
        "  # if the observed value is out of the max and min range assign it to the edge values\n",
        "  value = min(max(value, min_value), max_value)\n",
        "\n",
        "  # calculate the bin size\n",
        "  bin_size = (max_value - min_value) / num_bins\n",
        "\n",
        "  # calculate the bin index and bin index starts from 1\n",
        "  bin_index = int((value - min_value) / bin_size)\n",
        "\n",
        "  return bin_index\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  position, velocity = observation\n",
        "\n",
        "  state = [discretize(position, -1.2, 0.6, position_bins - 1), # discretize the position\n",
        "           discretize(velocity, -0.07, 0.07, velocity_bins - 1) # discretize the velocity\n",
        "           ]\n",
        "  return tuple(state)\n",
        "\n",
        "\n",
        "\n",
        "# initializing\n",
        "\n",
        "q = {} # action value function\n",
        "c = {} # sum of weights\n",
        "policy = {} # target policy\n",
        "b = {} # behaviour policy\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(position_bins),\n",
        "                           np.arange(velocity_bins))\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  c[state] = np.zeros((env.action_space.n)) # initialize the sum for the s-a pair\n",
        "  b[state] = np.full(((env.action_space.n)),1/env.action_space.n) # initialize the behaviour policy\n",
        "  q[state] = np.random.random((env.action_space.n)) # initialize the action-value function\n",
        "  policy[state] = np.zeros((env.action_space.n)) # initialize target policy with zero probability for all actions\n",
        "\n",
        "  best_action = np.argmax(q[state])  # selecting the best action based on the action-value function\n",
        "  policy[state][best_action] = 1  # setting 1 to the probability of the best action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTWpHWjl4Rb6",
        "outputId": "c8dd3d18-b590-440c-c145-34a26784eafc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  10000  time:  00:03:56\n",
            "Episode:  20000  time:  00:07:53\n",
            "Episode:  30000  time:  00:11:41\n",
            "Episode:  40000  time:  00:15:19\n",
            "Episode:  50000  time:  00:18:57\n",
            "Episode:  60000  time:  00:22:34\n",
            "Episode:  70000  time:  00:26:13\n",
            "Episode:  80000  time:  00:29:52\n",
            "Episode:  90000  time:  00:33:32\n",
            "Episode:  100000  time:  00:37:12\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# Iteration\n",
        "\n",
        "start_timer = time.time() # starting the timer\n",
        "\n",
        "\n",
        "epsilon = 0.1\n",
        "\n",
        "episodes = 1e5\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  # initializing a dictionary to save the episode's trajectory\n",
        "  episode_trajectory = [[0, state]] # starting reward (0) and state\n",
        "\n",
        "  # generating an episodes using a an epsilon greedy policy\n",
        "  for t in range(1,201):\n",
        "\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = b[state]) # selecting the action based on the behaviour policy\n",
        "\n",
        "    episode_trajectory[-1].append(action) # appending the action to the episode trajectory\n",
        "\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs) # discretize the observation\n",
        "    episode_trajectory.append([reward,state]) # append the reward and the state to the episode trajectory\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  # initializing the return and the weight\n",
        "  G = 0\n",
        "  W = 1\n",
        "  gamma = 0.9 # discount factor\n",
        "\n",
        "  # updating the action-value function\n",
        "  for t in range(len(episode_trajectory)-2,-1,-1):\n",
        "    G = gamma*G  + episode_trajectory[t+1][0]\n",
        "    state = episode_trajectory[t][1]\n",
        "    action = episode_trajectory[t][2]\n",
        "    c[state][action] += W  # updating the sum for the s-a pair\n",
        "    q[state][action] += (W/c[state][action]) * (G - q[state][action]) # incremental averaging\n",
        "\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "\n",
        "    policy[state] = np.zeros(env.action_space.n) # resetting the policy for the given state\n",
        "    policy[state][best_action] = 1 # setting the probability of the best action to 1\n",
        "\n",
        "\n",
        "    if action != best_action:\n",
        "      break\n",
        "    W = W/b[state][action]\n",
        "\n",
        "  # updating the behavioural policy b to be the epsilon soft policy of the target policy\n",
        "  for state in states:\n",
        "    best_action = np.argmax(q[state])\n",
        "    b[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "    b[state][best_action] += 1-epsilon\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 10000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnn8J86lVema",
        "outputId": "f38d1d1c-db5d-40b8-d8e9-92ea1c54abee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 reward:  0  state:  (3, 4)  action= 1\n",
            "step 1 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 2 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 3 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 4 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 5 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 6 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 7 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 8 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 9 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 10 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 11 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 12 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 13 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 14 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 15 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 16 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 17 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 18 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 19 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 20 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 21 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 22 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 23 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 24 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 25 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 26 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 27 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 28 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 29 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 30 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 31 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 32 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 33 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 34 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 35 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 36 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 37 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 38 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 39 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 40 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 41 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 42 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 43 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 44 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 45 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 46 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 47 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 48 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 49 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 50 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 51 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 52 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 53 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 54 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 55 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 56 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 57 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 58 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 59 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 60 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 61 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 62 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 63 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 64 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 65 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 66 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 67 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 68 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 69 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 70 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 71 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 72 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 73 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 74 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 75 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 76 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 77 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 78 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 79 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 80 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 81 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 82 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 83 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 84 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 85 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 86 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 87 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 88 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 89 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 90 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 91 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 92 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 93 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 94 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 95 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 96 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 97 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 98 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 99 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 100 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 101 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 102 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 103 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 104 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 105 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 106 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 107 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 108 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 109 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 110 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 111 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 112 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 113 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 114 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 115 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 116 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 117 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 118 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 119 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 120 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 121 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 122 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 123 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 124 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 125 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 126 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 127 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 128 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 129 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 130 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 131 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 132 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 133 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 134 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 135 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 136 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 137 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 138 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 139 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 140 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 141 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 142 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 143 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 144 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 145 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 146 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 147 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 148 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 149 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 150 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 151 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 152 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 153 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 154 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 155 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 156 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 157 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 158 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 159 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 160 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 161 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 162 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 163 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 164 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 165 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 166 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 167 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 168 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 169 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 170 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 171 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 172 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 173 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 174 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 175 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 176 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 177 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 178 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 179 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 180 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 181 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 182 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 183 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 184 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 185 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 186 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 187 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 188 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 189 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 190 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 191 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 192 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 193 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 194 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 195 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 196 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 197 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 198 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "step 199 reward:  -1.0  state:  (3, 4)  action= 1\n",
            "done\n",
            "Iterations that were run: 199\n"
          ]
        }
      ],
      "source": [
        "obs = env.reset()\n",
        "state = discretize_observation(obs)\n",
        "reward = 0\n",
        "for i in range(50000):\n",
        "  action = np.argmax(policy[state])\n",
        "  print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "\n",
        "  state = discretize_observation(obs)\n",
        "\n",
        "  if done:\n",
        "    print (\"done\")\n",
        "    break\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYI_9Bc-VwGT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
