{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAWtQeE1YmI"
      },
      "source": [
        "@author: Gediyon M. Girma\n",
        "\n",
        "on-policy expected SARSA control for MountainCar environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n8M9lo2Uqi5Z"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zid1tNK61jO9"
      },
      "outputs": [],
      "source": [
        "# discretization of the observation space\n",
        "\n",
        "bins = (30, 30)\n",
        "\n",
        "def discretize_observation(observation):\n",
        "  \"\"\"\n",
        "  Discretizes the observation space.\n",
        "\n",
        "  Args:\n",
        "    observation: The observation to discretize.\n",
        "\n",
        "  Returns:\n",
        "    The discretized observation.\n",
        "  \"\"\"\n",
        "\n",
        "  low = env.observation_space.low\n",
        "  high = env.observation_space.high\n",
        "\n",
        "  return tuple(np.digitize(observation[i], np.linspace(low[i], high[i], bins[i] + 1)) - 1 for i in range(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lmj2qU0ww3to"
      },
      "outputs": [],
      "source": [
        "# the on-policy SARSA\n",
        "alpha = 0.1\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "\n",
        "# formaulate the state space with every combination of the discritsized elements of the states\n",
        "states = itertools.product(np.arange(bins[0]), np.arange(bins[1]))\n",
        "\n",
        "q={}\n",
        "policy = {}\n",
        "\n",
        "\n",
        "for state in states:\n",
        "  q[state] = np.zeros((env.action_space.n)) # initialize the action-value function\n",
        "\n",
        "  # initializing the epsilon-greedy policy\n",
        "  policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n)\n",
        "  policy[state][np.random.randint(3)] += 1-epsilon\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxjJDYan1mJR",
        "outputId": "ec75b20e-1710-47ae-ccd0-ccbb2ce16696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:  1000  time:  00:00:21\n",
            "Episode:  2000  time:  00:00:45\n",
            "Episode:  3000  time:  00:01:06\n",
            "Episode:  4000  time:  00:01:28\n",
            "Episode:  5000  time:  00:01:50\n",
            "Episode:  6000  time:  00:02:11\n",
            "Episode:  7000  time:  00:02:36\n",
            "Episode:  8000  time:  00:02:58\n",
            "Episode:  9000  time:  00:03:19\n",
            "Episode:  10000  time:  00:03:40\n",
            "Episode:  11000  time:  00:04:04\n",
            "Episode:  12000  time:  00:04:29\n",
            "Episode:  13000  time:  00:04:51\n",
            "Episode:  14000  time:  00:05:11\n",
            "Episode:  15000  time:  00:05:32\n",
            "Episode:  16000  time:  00:05:55\n",
            "Episode:  17000  time:  00:06:16\n",
            "Episode:  18000  time:  00:06:38\n",
            "Episode:  19000  time:  00:07:00\n",
            "Episode:  20000  time:  00:07:22\n",
            "Episode:  21000  time:  00:07:42\n",
            "Episode:  22000  time:  00:08:03\n",
            "Episode:  23000  time:  00:08:24\n",
            "Episode:  24000  time:  00:08:45\n",
            "Episode:  25000  time:  00:09:06\n",
            "Episode:  26000  time:  00:09:27\n",
            "Episode:  27000  time:  00:09:47\n",
            "Episode:  28000  time:  00:10:08\n",
            "Episode:  29000  time:  00:10:29\n",
            "Episode:  30000  time:  00:10:50\n",
            "Episode:  31000  time:  00:11:10\n",
            "Episode:  32000  time:  00:11:30\n",
            "Episode:  33000  time:  00:11:50\n",
            "Episode:  34000  time:  00:12:12\n",
            "Episode:  35000  time:  00:12:33\n",
            "Episode:  36000  time:  00:12:54\n",
            "Episode:  37000  time:  00:13:14\n",
            "Episode:  38000  time:  00:13:35\n",
            "Episode:  39000  time:  00:13:56\n",
            "Episode:  40000  time:  00:14:16\n",
            "Episode:  41000  time:  00:14:37\n",
            "Episode:  42000  time:  00:14:59\n",
            "Episode:  43000  time:  00:15:18\n",
            "Episode:  44000  time:  00:15:39\n",
            "Episode:  45000  time:  00:16:00\n",
            "Episode:  46000  time:  00:16:20\n",
            "Episode:  47000  time:  00:16:41\n",
            "Episode:  48000  time:  00:17:01\n",
            "Episode:  49000  time:  00:17:22\n",
            "Episode:  50000  time:  00:17:43\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        " # training\n",
        "episodes = 5e4\n",
        "start_timer = time.time()\n",
        "episode = 1\n",
        "while episode < episodes:\n",
        "\n",
        "  # reset the environment\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs) # discretize the observation\n",
        "\n",
        "  for t in range(1, 500):\n",
        "    action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # select an action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = discretize_observation(obs) # next state\n",
        "    next_action = np.random.choice(np.arange(env.action_space.n), p = policy[state]) # next action\n",
        "\n",
        "    Expected_value = sum([policy[next_state][action]*q[next_state][next_action]\n",
        "                          for action in range(env.action_space.n)])\n",
        "    q[state][action] += alpha * (reward + gamma * Expected_value - q[state][action]) # update the action-value function\n",
        "\n",
        "    # update the policy based on the action-value function\n",
        "    best_action = np.argmax(q[state]) # selecting the best action based on the action-value function\n",
        "    policy[state] = np.full(((env.action_space.n)),epsilon/env.action_space.n) # resetting the policy for the given state\n",
        "    policy[state][best_action] += 1 - epsilon # setting the probability of the best action to 1\n",
        "\n",
        "    state = next_state\n",
        "    action = next_action\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "\n",
        "  episode += 1\n",
        "  if episode % 1000 == 0:\n",
        "    end_timer = time.time()\n",
        "    timer = end_timer - start_timer\n",
        "    elapsed_time_struct = time.gmtime(timer)\n",
        "    formatted_time = time.strftime(\"%H:%M:%S\", elapsed_time_struct)\n",
        "    print(\"Episode: \",episode, \" time: \", formatted_time)\n",
        "\n",
        "  if episode == episodes:\n",
        "    print(\"done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzIPr5MAYaj9",
        "outputId": "61df4e42-d874-42b1-83ba-80afa0a0fd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[107, 125, 108, 155, 131, 107, 148, 137, 145, 146, 107, 131, 108, 149, 148, 149, 107, 131, 130, 108, 149, 153, 141, 127, 129, 131, 147, 107, 149, 131, 148, 127, 128, 147, 155, 148, 148, 130, 152, 148, 149, 129, 131, 127, 129, 141, 126, 132, 107, 129, 131, 150, 107, 128, 107, 153, 152, 125, 107, 147, 131, 130, 107, 129, 146, 129, 108, 129, 107, 127, 146, 149, 107, 107, 107, 107, 107, 142, 148, 153, 132, 151, 147, 154, 128, 155, 107, 107, 149, 149, 127, 108, 128, 148, 132, 150, 150, 107, 130, 144]\n",
            "Average steps to solution:  131.98\n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "steps_to_solution = []\n",
        "\n",
        "for j in range(100):\n",
        "  obs = env.reset()\n",
        "  state = discretize_observation(obs)\n",
        "  for i in range(1,50000):\n",
        "    action = np.argmax(policy[state])\n",
        "    # print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    state = discretize_observation(obs)\n",
        "\n",
        "    if done:\n",
        "      #print (\"done\")\n",
        "      steps_to_solution.append(i)\n",
        "      break\n",
        "print(steps_to_solution)\n",
        "avg_step = np.mean(steps_to_solution)\n",
        "print(\"Average steps to solution: \",avg_step)\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7eLhXs3Ncn",
        "outputId": "d9887367-0299-460c-d755-75a636d18845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 reward:  0  state:  (12, 15)  action= 0\n",
            "step 1 reward:  -1.0  state:  (12, 14)  action= 0\n",
            "step 2 reward:  -1.0  state:  (12, 14)  action= 0\n",
            "step 3 reward:  -1.0  state:  (12, 14)  action= 0\n",
            "step 4 reward:  -1.0  state:  (12, 13)  action= 0\n",
            "step 5 reward:  -1.0  state:  (11, 13)  action= 0\n",
            "step 6 reward:  -1.0  state:  (11, 13)  action= 0\n",
            "step 7 reward:  -1.0  state:  (11, 12)  action= 0\n",
            "step 8 reward:  -1.0  state:  (11, 12)  action= 0\n",
            "step 9 reward:  -1.0  state:  (11, 12)  action= 0\n",
            "step 10 reward:  -1.0  state:  (11, 12)  action= 0\n",
            "step 11 reward:  -1.0  state:  (10, 12)  action= 0\n",
            "step 12 reward:  -1.0  state:  (10, 11)  action= 0\n",
            "step 13 reward:  -1.0  state:  (10, 11)  action= 0\n",
            "step 14 reward:  -1.0  state:  (10, 11)  action= 0\n",
            "step 15 reward:  -1.0  state:  (9, 11)  action= 0\n",
            "step 16 reward:  -1.0  state:  (9, 11)  action= 0\n",
            "step 17 reward:  -1.0  state:  (9, 11)  action= 0\n",
            "step 18 reward:  -1.0  state:  (8, 11)  action= 0\n",
            "step 19 reward:  -1.0  state:  (8, 11)  action= 0\n",
            "step 20 reward:  -1.0  state:  (8, 11)  action= 0\n",
            "step 21 reward:  -1.0  state:  (8, 11)  action= 0\n",
            "step 22 reward:  -1.0  state:  (7, 11)  action= 0\n",
            "step 23 reward:  -1.0  state:  (7, 11)  action= 0\n",
            "step 24 reward:  -1.0  state:  (7, 11)  action= 0\n",
            "step 25 reward:  -1.0  state:  (7, 11)  action= 0\n",
            "step 26 reward:  -1.0  state:  (6, 11)  action= 0\n",
            "step 27 reward:  -1.0  state:  (6, 12)  action= 0\n",
            "step 28 reward:  -1.0  state:  (6, 12)  action= 0\n",
            "step 29 reward:  -1.0  state:  (6, 12)  action= 0\n",
            "step 30 reward:  -1.0  state:  (6, 12)  action= 0\n",
            "step 31 reward:  -1.0  state:  (5, 12)  action= 2\n",
            "step 32 reward:  -1.0  state:  (5, 13)  action= 1\n",
            "step 33 reward:  -1.0  state:  (5, 14)  action= 1\n",
            "step 34 reward:  -1.0  state:  (5, 14)  action= 1\n",
            "step 35 reward:  -1.0  state:  (5, 14)  action= 1\n",
            "step 36 reward:  -1.0  state:  (5, 15)  action= 0\n",
            "step 37 reward:  -1.0  state:  (5, 15)  action= 0\n",
            "step 38 reward:  -1.0  state:  (5, 15)  action= 0\n",
            "step 39 reward:  -1.0  state:  (5, 16)  action= 2\n",
            "step 40 reward:  -1.0  state:  (6, 16)  action= 2\n",
            "step 41 reward:  -1.0  state:  (6, 17)  action= 2\n",
            "step 42 reward:  -1.0  state:  (6, 18)  action= 2\n",
            "step 43 reward:  -1.0  state:  (6, 18)  action= 2\n",
            "step 44 reward:  -1.0  state:  (7, 19)  action= 2\n",
            "step 45 reward:  -1.0  state:  (7, 19)  action= 2\n",
            "step 46 reward:  -1.0  state:  (7, 20)  action= 2\n",
            "step 47 reward:  -1.0  state:  (8, 20)  action= 2\n",
            "step 48 reward:  -1.0  state:  (8, 21)  action= 2\n",
            "step 49 reward:  -1.0  state:  (9, 21)  action= 2\n",
            "step 50 reward:  -1.0  state:  (9, 22)  action= 0\n",
            "step 51 reward:  -1.0  state:  (10, 22)  action= 1\n",
            "step 52 reward:  -1.0  state:  (11, 22)  action= 2\n",
            "step 53 reward:  -1.0  state:  (11, 22)  action= 2\n",
            "step 54 reward:  -1.0  state:  (12, 22)  action= 2\n",
            "step 55 reward:  -1.0  state:  (12, 22)  action= 2\n",
            "step 56 reward:  -1.0  state:  (13, 22)  action= 2\n",
            "step 57 reward:  -1.0  state:  (14, 22)  action= 2\n",
            "step 58 reward:  -1.0  state:  (14, 22)  action= 2\n",
            "step 59 reward:  -1.0  state:  (15, 22)  action= 0\n",
            "step 60 reward:  -1.0  state:  (15, 22)  action= 0\n",
            "step 61 reward:  -1.0  state:  (16, 21)  action= 2\n",
            "step 62 reward:  -1.0  state:  (16, 21)  action= 2\n",
            "step 63 reward:  -1.0  state:  (17, 21)  action= 2\n",
            "step 64 reward:  -1.0  state:  (17, 20)  action= 2\n",
            "step 65 reward:  -1.0  state:  (18, 20)  action= 2\n",
            "step 66 reward:  -1.0  state:  (18, 20)  action= 2\n",
            "step 67 reward:  -1.0  state:  (19, 19)  action= 2\n",
            "step 68 reward:  -1.0  state:  (19, 19)  action= 2\n",
            "step 69 reward:  -1.0  state:  (19, 19)  action= 2\n",
            "step 70 reward:  -1.0  state:  (20, 19)  action= 0\n",
            "step 71 reward:  -1.0  state:  (20, 18)  action= 2\n",
            "step 72 reward:  -1.0  state:  (20, 17)  action= 0\n",
            "step 73 reward:  -1.0  state:  (20, 17)  action= 0\n",
            "step 74 reward:  -1.0  state:  (20, 16)  action= 0\n",
            "step 75 reward:  -1.0  state:  (20, 15)  action= 0\n",
            "step 76 reward:  -1.0  state:  (20, 14)  action= 0\n",
            "step 77 reward:  -1.0  state:  (20, 14)  action= 0\n",
            "step 78 reward:  -1.0  state:  (20, 13)  action= 0\n",
            "step 79 reward:  -1.0  state:  (20, 12)  action= 0\n",
            "step 80 reward:  -1.0  state:  (20, 11)  action= 0\n",
            "step 81 reward:  -1.0  state:  (19, 11)  action= 0\n",
            "step 82 reward:  -1.0  state:  (19, 10)  action= 0\n",
            "step 83 reward:  -1.0  state:  (19, 9)  action= 0\n",
            "step 84 reward:  -1.0  state:  (18, 8)  action= 0\n",
            "step 85 reward:  -1.0  state:  (18, 8)  action= 0\n",
            "step 86 reward:  -1.0  state:  (17, 7)  action= 0\n",
            "step 87 reward:  -1.0  state:  (16, 6)  action= 0\n",
            "step 88 reward:  -1.0  state:  (16, 6)  action= 0\n",
            "step 89 reward:  -1.0  state:  (15, 5)  action= 0\n",
            "step 90 reward:  -1.0  state:  (14, 4)  action= 2\n",
            "step 91 reward:  -1.0  state:  (13, 4)  action= 1\n",
            "step 92 reward:  -1.0  state:  (13, 4)  action= 1\n",
            "step 93 reward:  -1.0  state:  (12, 4)  action= 0\n",
            "step 94 reward:  -1.0  state:  (11, 4)  action= 0\n",
            "step 95 reward:  -1.0  state:  (10, 3)  action= 0\n",
            "step 96 reward:  -1.0  state:  (9, 3)  action= 0\n",
            "step 97 reward:  -1.0  state:  (8, 3)  action= 0\n",
            "step 98 reward:  -1.0  state:  (7, 3)  action= 0\n",
            "step 99 reward:  -1.0  state:  (7, 3)  action= 0\n",
            "step 100 reward:  -1.0  state:  (6, 3)  action= 0\n",
            "step 101 reward:  -1.0  state:  (5, 4)  action= 0\n",
            "step 102 reward:  -1.0  state:  (4, 4)  action= 0\n",
            "step 103 reward:  -1.0  state:  (3, 4)  action= 0\n",
            "step 104 reward:  -1.0  state:  (2, 4)  action= 0\n",
            "step 105 reward:  -1.0  state:  (2, 5)  action= 0\n",
            "step 106 reward:  -1.0  state:  (1, 5)  action= 2\n",
            "step 107 reward:  -1.0  state:  (0, 6)  action= 2\n",
            "step 108 reward:  -1.0  state:  (0, 7)  action= 2\n",
            "step 109 reward:  -1.0  state:  (0, 15)  action= 2\n",
            "step 110 reward:  -1.0  state:  (0, 15)  action= 2\n",
            "step 111 reward:  -1.0  state:  (0, 16)  action= 2\n",
            "step 112 reward:  -1.0  state:  (0, 17)  action= 2\n",
            "step 113 reward:  -1.0  state:  (0, 17)  action= 2\n",
            "step 114 reward:  -1.0  state:  (0, 18)  action= 2\n",
            "step 115 reward:  -1.0  state:  (1, 19)  action= 2\n",
            "step 116 reward:  -1.0  state:  (1, 19)  action= 2\n",
            "step 117 reward:  -1.0  state:  (1, 20)  action= 2\n",
            "step 118 reward:  -1.0  state:  (2, 21)  action= 2\n",
            "step 119 reward:  -1.0  state:  (3, 22)  action= 2\n",
            "step 120 reward:  -1.0  state:  (3, 22)  action= 2\n",
            "step 121 reward:  -1.0  state:  (4, 23)  action= 2\n",
            "step 122 reward:  -1.0  state:  (5, 24)  action= 2\n",
            "step 123 reward:  -1.0  state:  (5, 25)  action= 2\n",
            "step 124 reward:  -1.0  state:  (6, 25)  action= 2\n",
            "step 125 reward:  -1.0  state:  (7, 26)  action= 2\n",
            "step 126 reward:  -1.0  state:  (8, 26)  action= 2\n",
            "step 127 reward:  -1.0  state:  (9, 27)  action= 2\n",
            "step 128 reward:  -1.0  state:  (10, 27)  action= 2\n",
            "step 129 reward:  -1.0  state:  (11, 28)  action= 2\n",
            "step 130 reward:  -1.0  state:  (12, 28)  action= 2\n",
            "step 131 reward:  -1.0  state:  (13, 28)  action= 2\n",
            "step 132 reward:  -1.0  state:  (14, 28)  action= 2\n",
            "step 133 reward:  -1.0  state:  (15, 28)  action= 2\n",
            "step 134 reward:  -1.0  state:  (16, 28)  action= 2\n",
            "step 135 reward:  -1.0  state:  (17, 27)  action= 2\n",
            "step 136 reward:  -1.0  state:  (18, 27)  action= 2\n",
            "step 137 reward:  -1.0  state:  (19, 27)  action= 2\n",
            "step 138 reward:  -1.0  state:  (20, 26)  action= 2\n",
            "step 139 reward:  -1.0  state:  (21, 26)  action= 2\n",
            "step 140 reward:  -1.0  state:  (22, 26)  action= 2\n",
            "step 141 reward:  -1.0  state:  (23, 26)  action= 2\n",
            "step 142 reward:  -1.0  state:  (24, 25)  action= 2\n",
            "step 143 reward:  -1.0  state:  (24, 25)  action= 2\n",
            "step 144 reward:  -1.0  state:  (25, 25)  action= 2\n",
            "step 145 reward:  -1.0  state:  (26, 25)  action= 2\n",
            "step 146 reward:  -1.0  state:  (27, 25)  action= 2\n",
            "step 147 reward:  -1.0  state:  (28, 25)  action= 1\n",
            "done\n",
            "Iterations that were run: 147\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "obs = env.reset()\n",
        "state = discretize_observation(obs)\n",
        "reward = 0\n",
        "for i in range(50000):\n",
        "  action = np.argmax(policy[state])\n",
        "  print(\"step\",i, \"reward: \", reward,\" state: \", state,\" action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  state = discretize_observation(obs)\n",
        "\n",
        "  if done:\n",
        "    print (\"done\")\n",
        "    break\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
